{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from scHPL import train, predict, update, progressive_learning, utils, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "#### Reading a csv file\n",
    "During this tutorial, we will work with the simulated data. This dataset can be downloaded from the Zenodo repository (https://doi.org/10.5281/zenodo.4557712)\n",
    "\n",
    "For scHPL, the input format of the data needs to be a pandas dataframe (rows: cells, columns: genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Simulated_data.csv', index_col = 0)\n",
    "data = data.T\n",
    "\n",
    "labels = pd.read_csv('Simulated_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anndata objects\n",
    "\n",
    "If you're working with an Anndata object, this can be transformed into a pandas dataframe using the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata_object = sc.read('anndata.h5ad')\n",
    "data = pd.DataFrame(data = adata_object.X, index = adata_object.obs_names, columns=adata_object.var_names)\n",
    "labels = pd.DataFrame(data = adata_object.obs['labels'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scHPL without progressive learning\n",
    "\n",
    "Here, we explain how to train a classifier without progressive learning, which can be used when the hierarchy is known beforehand.\n",
    "\n",
    "First we split the dataset into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a tree\n",
    "\n",
    "We create a tree using the utils.create_tree() function. This function creates a tree based on the newick format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = utils.create_tree('((Group1, Group2)Group12, Group3,(Group4, (Group5, Group6)Group56)Group456)root')\n",
    "utils.print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the tree\n",
    "\n",
    "Next, we train this tree. There are different options here:\n",
    "    - classifier: can be either 'svm_occ' for the one_class svm or 'svm' for the linear SVM\n",
    "    - dimred: whether to apply dimensionality reduction to select features. \n",
    "      For the one-class SVM, this is recommended. \n",
    "      For the linear SVM, it is recommended to turn off and rely on the built-in L2-regularization\n",
    "    - useRE: whether cells are rejected based on the reconstruction error\n",
    "    - FN: percentage of false negatives allowed when using the reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = train.train_tree(x_train, y_train, tree, classifier = 'svm_occ', dimred = True, useRE = True, FN = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scHPL with progressive learning\n",
    "\n",
    "scHPL can be used to learn a hierarchy of cell populations by combining the annotations of different datasets. scHPL, however, is not robust to batch effects between the datasets. We recommend to align the datasets before using scHPL\n",
    "\n",
    "#### Preprocessing the simulated data\n",
    "\n",
    "We will again split the data in a train and test dataset. We will split the training dataset again in 3 batches to simulate different datasets. To simulate the effect of different resolutions in these batches, we have to relabel some of the populations (e.g. 'Group1' and 'Group2' are renamed as 'Group12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "sss = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 0)\n",
    "sss.get_n_splits(x_train, y_train)\n",
    "\n",
    "x_batches = []\n",
    "y_batches = []\n",
    "\n",
    "for trainindex, testindex in sss.split(x_train, y_train):\n",
    "    x_batches.append(x_train.iloc[testindex])\n",
    "    y_batches.append(y_train.iloc[testindex])\n",
    "\n",
    "# In batch 1, we merge group1 and group2\n",
    "xx = np.where((y_batches[0] == 'Group1') | (y_batches[0] == 'Group2'))[0]\n",
    "y_batches[0].values[xx] = 'Group12'\n",
    "\n",
    "# In batch 1, we merge group4 and group5 and group6\n",
    "xx = np.where((y_batches[0] == 'Group4') | (y_batches[0] == 'Group5') | (y_batches[0] == 'Group6'))[0]\n",
    "y_batches[0].values[xx] = 'Group456'\n",
    "\n",
    "# In batch 2, we merge group5 and group6\n",
    "xx = np.where((y_batches[1] == 'Group5') | (y_batches[1] == 'Group6'))[0]\n",
    "y_batches[1].values[xx] = 'Group56'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and training the tree\n",
    "\n",
    "We learn the tree using the progressive_learning.learn_tree() function. \n",
    "There are different input parameters here:\n",
    "    - data: array of datasets. The first dimension is the number of datasets. Each dataset is a pandas DataFrame as described in the Data Loading section. \n",
    "    - labels: array of labels belonging to the datasets. The first dimension is the number of datasets.\n",
    "    - classifier: can be either 'svm_occ' for the one_class svm or 'svm' for the linear SVM\n",
    "    - dimred: whether to apply dimensionality reduction to select features. \n",
    "      For the one-class SVM, this is recommended. \n",
    "      For the linear SVM, it is ecommended to turn off and rely on the built-in L2-regularization\n",
    "    - useRE: whether cells are rejected based on the reconstruction error\n",
    "    - FN: percentage of false negatives allowed when using the reconstruction error\n",
    "    - threshold: matching threshold (default = 0.25)\n",
    "    - return_missing: populations that caused a complex scenario and are not added to the tree can be returned to the user (return_missing = True) or attached to the root node (return_missing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = progressive_learning.learn_tree(x_batches, y_batches, classifier = 'svm_occ', dimred = True, useRE = True, \n",
    "                  FN = 1, threshold = 0.25, return_missing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels\n",
    "\n",
    "A trained tree (with or withour progressive learning) can be used to predict the labels of another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict.predict_labels(x_test, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Here, we evaluate the predictions based on the hierarchical F1-score and look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF1_score = evaluate.hierarchical_F1(y_test.values, y_pred, tree)\n",
    "confmatrix = evaluate.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HF1_score)\n",
    "print(confmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful functions\n",
    "\n",
    "Using the utils functions, nodes in the tree can be added, removed, or renamed. Note that after adding or removing a node, the tree has to be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tree:\n",
      "root\n",
      "\tGroup12\n",
      "\t\tGroup1\n",
      "\t\tGroup2\n",
      "\tGroup3\n",
      "\tGroup456\n",
      "\t\tGroup4\n",
      "\t\tGroup56\n",
      "\t\t\tGroup5\n",
      "\t\t\tGroup6\n",
      "Tree after adding the new node:\n",
      "root\n",
      "\tGroup12\n",
      "\t\tGroup1\n",
      "\t\tGroup2\n",
      "\t\t\textra node\n",
      "\tGroup3\n",
      "\tGroup456\n",
      "\t\tGroup4\n",
      "\t\tGroup56\n",
      "\t\t\tGroup5\n",
      "\t\t\tGroup6\n",
      "Tree after removing the node:\n",
      "root\n",
      "\tGroup12\n",
      "\t\tGroup1\n",
      "\t\tGroup2\n",
      "\t\t\textra node\n",
      "\tGroup3\n",
      "\tGroup456\n",
      "\t\tGroup4\n",
      "\t\tGroup5\n",
      "\t\tGroup6\n",
      "Tree after renaming the node:\n",
      "root\n",
      "\tnew name\n",
      "\t\tGroup1\n",
      "\t\tGroup2\n",
      "\t\t\textra node\n",
      "\tGroup3\n",
      "\tGroup456\n",
      "\t\tGroup4\n",
      "\t\tGroup5\n",
      "\t\tGroup6\n"
     ]
    }
   ],
   "source": [
    "tree = utils.create_tree('((Group1, Group2)Group12, Group3,(Group4, (Group5, Group6)Group56)Group456)root')\n",
    "\n",
    "print('Original tree:')\n",
    "utils.print_tree(tree)\n",
    "\n",
    "# Now we add a node to the tree\n",
    "tree = utils.add_node(name = 'extra node', tree = tree, parent = 'Group2')\n",
    "print('Tree after adding the new node:')\n",
    "utils.print_tree(tree)\n",
    "\n",
    "# Now we remove a node from the tree\n",
    "# Children = False, indicates that the children should not be removed\n",
    "tree = utils.remove_node(name = 'Group56', tree = tree, children = False)\n",
    "print('Tree after removing the node:')\n",
    "utils.print_tree(tree)\n",
    "\n",
    "# We rename a node\n",
    "tree = utils.rename_node(old_name = 'Group12', new_name = 'new name', tree = tree)\n",
    "print('Tree after renaming the node:')\n",
    "utils.print_tree(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
